{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "max_iters = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs are available.\n",
      "Number of GPUs: 1\n",
      "Device is set to: cuda\n",
      "GPU 0: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(\"GPUs are available.\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"Device is set to: {device}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"No GPUs available.\")\n",
    "        device = 'cpu'\n",
    "        print(f\"Device is set to: {device}\")\n",
    "    return device\n",
    "\n",
    "device = check_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 \n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r' , encoding='utf-8') as f:\n",
    "    text= f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i ,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i ,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,  0,\n",
      "         0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0,\n",
      "         1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25,\n",
      "        38, 28,  1, 39, 30,  1, 39, 50,  9,  1])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n] \n",
    "val_data = data[n:] \n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size]for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
    "    return x, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) target is tensor(80)\n",
      "when input is tensor([80, 28]) target is tensor(28)\n",
      "when input is tensor([80, 28, 39]) target is tensor(39)\n",
      "when input is tensor([80, 28, 39, 42]) target is tensor(42)\n",
      "when input is tensor([80, 28, 39, 42, 39]) target is tensor(39)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44]) target is tensor(44)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44, 32]) target is tensor(32)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44, 32, 49]) target is tensor(49)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 \n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = x[t]\n",
    "    print('when input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([122419,  67417,  12114,  90497])\n",
      "The shape of x is: torch.Size([4, 8])\n",
      "x is: tensor([[58, 72,  9,  1, 54, 67, 57,  1],\n",
      "        [ 1, 67, 58, 77, 73,  9,  1, 55],\n",
      "        [58, 62, 73, 61, 58, 71,  1, 73],\n",
      "        [72,  1, 73, 61, 58,  1, 46, 54]], device='cuda:0')\n",
      "\n",
      "The shape of y is: torch.Size([4, 8])\n",
      "y is: tensor([[72,  9,  1, 54, 67, 57,  1, 58],\n",
      "        [67, 58, 77, 73,  9,  1, 55, 74],\n",
      "        [62, 73, 61, 58, 71,  1, 73, 61],\n",
      "        [ 1, 73, 61, 58,  1, 46, 54, 65]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "x,y = x.to(device) , y.to(device)\n",
    "print(f\"The shape of x is: {x.shape}\")\n",
    "print(f\"x is: {x}\",  end='\\n\\n')\n",
    "print(f\"The shape of y is: {y.shape}\")\n",
    "print(f\"y is: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3WGSfh1N3DWSA\"\n",
      "t2(jo oRv8MyG*Zpva*9k mUEXNygdC87﻿52]7XjlJER,RzW6P8yocl(x_S'smwXuVSRUo?XXa\"n[sKrv3G5J,S*53dKhGl6hTY7(cVsxa&!EB\n",
      "AhM\"w&aE .GF3,OZZ;Nxp020TUmHTjb8X7H-viNP2Ti7XNO;'FXY7vi72OKL3:iR)u(\"ObR1aGgPHOgNJ,Vwf3Ow&V[k.Y9q2ZA& cu7E*P;fA﻿ I'8u﻿oSBzDr5*WN\n",
      "I);Z2wKk'sY:jMfr[brP9zaamwXd-GN]M0RD1b*QEYXDrviS1q8v!iM*3O)zG'yds8dmBV2TWR(MbKI\n",
      "'1\n",
      "z﻿oaKLi[kbp9ozs\n",
      "AcDxt EY9]:q1\n",
      "zT]y!﻿Mx﻿7mlkXhq0fN)ct(xmAjlf8yi35;9IrA:'F&wKx1\n",
      "fk.6g9O;zHe\"&hmtBUmxWZ(1\"FNLM(&'Fw1\n",
      "X[G_dQ\n",
      "(6McU;NPzt1\n",
      "gf*XD-G76niiWyWWP;5;U,;dsSdv1!\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim= -1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index,index_next), dim=1)\n",
    "\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.378742218017578\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pey t l izeds thand ks wecaref nt ais k mid he I s acit'sire be ily thio grangos.\"\n",
      "\n",
      "T but gnoos  urnde, tr mshe Buthe Oz y  he pung tre higly. acatilins,\n",
      "S L.\n",
      "\n",
      "s the thos ce PTh tourewe gruthed ougeve h cerongy, m. boothe ainso; HEutthe  Zed theden  m\n",
      "tord the upino\n",
      "hryonellsaluther tamyod, nglan he mbeatw OSum thistind. thato icowe thinthive ikis touly\n",
      "T fis wereswomud ataruteed  peslde fre mo swh.\n",
      "\n",
      "\" there hengowis o whed. thyo ws brelitisaprra, iry\n",
      "\n",
      "as cle, seanoman \"ong. tis. we, tes asam f\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
